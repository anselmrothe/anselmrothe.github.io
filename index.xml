<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Anselm Rothe on Anselm Rothe</title>
    <link>https://anselmrothe.github.io/</link>
    <description>Recent content in Anselm Rothe on Anselm Rothe</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018</copyright>
    <lastBuildDate>Sun, 15 Oct 2017 00:00:00 +0200</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title></title>
      <link>https://anselmrothe.github.io/post/mole_cogsci2020/</link>
      <pubDate>Sun, 19 Jul 2020 00:00:00 +0200</pubDate>
      
      <guid>https://anselmrothe.github.io/post/mole_cogsci2020/</guid>
      <description>

&lt;p&gt;&lt;base target=&#34;_blank&#34;&gt;&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&lt;strong&gt;online conference poster&lt;/strong&gt; for CogSci2020&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;h2 id=&#34;learning-sequential-patterns-from-graphical-programs&#34;&gt;&lt;strong&gt;Learning sequential patterns from graphical programs&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;A fundamental question in cognitive science is how people infer structured, rule-like mental representations (Fodor, 1975).&lt;/p&gt;

&lt;p&gt;We are addressing this question by introducing an experimental paradigm called “Track-A-Mole”, in which participants predict the step-wise movements of a cartoon mole on a two-dimensional space.&lt;/p&gt;

&lt;p&gt;The movements of the mole are generated by underlying graphical programs.&lt;/p&gt;

&lt;p&gt;Figure 1: (&lt;a href=&#34;https://anselmrothe.github.io/mole_cogsci2020/img/fig1.pdf&#34;&gt;PDF version&lt;/a&gt;)
&lt;img src=&#34;https://anselmrothe.github.io/mole_cogsci2020/img/fig1.png&#34; alt=&#34;Interface and patterns&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;graphical-programs-for-sequential-patterns&#34;&gt;Graphical programs for sequential patterns&lt;/h3&gt;

&lt;p&gt;The programs are built from a few meaningful building blocks, such as &lt;em&gt;jump&lt;/em&gt;, which moves the mole along its current direction and &lt;em&gt;turn&lt;/em&gt;, which changes its current direction. All programs are wrapped in a for-loop, which executes the code inside repeatedly, for example &lt;em&gt;for_i(jump, turn)&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;With a mixture of continuous and symbolic features (e.g., various jump sizes, angular changes of direction, countable repetitions, and loops) that are governing these patterns and that have to be inferred by people, this domain is richer than previously studied paradigms (e.g., Amalric et al., 2017).&lt;/p&gt;

&lt;h3 id=&#34;results&#34;&gt;Results&lt;/h3&gt;

&lt;p&gt;96 participants were tested on Amazon Mechanical Turk. Many people inferred the patterns quickly (i.e., after observing a few steps; e.g., see the pattern “square” in Figure 2 below).&lt;/p&gt;

&lt;p&gt;When people made errors, they were usually not random (e.g., participants predicted the mole to appear in a corner but the wrong one; see the pattern “switching sides”).&lt;/p&gt;

&lt;p&gt;People seemed to assume a more parsimonious pattern structure at first, which they then updated to a more complex one later on (see “growing sinusoid” and “sparse spiral”).&lt;/p&gt;

&lt;p&gt;Figure 2: (&lt;a href=&#34;https://anselmrothe.github.io/mole_cogsci2020/img/fig2.pdf&#34;&gt;PDF version&lt;/a&gt;)
&lt;img src=&#34;https://anselmrothe.github.io/mole_cogsci2020/img/fig2.png&#34; alt=&#34;Results&#34; /&gt;&lt;/p&gt;

&lt;!-- TODO (see error in firefox console): Click [here](/mole_cogsci2020/mole2-try-it-out-html/exp.html) to try it out. --&gt;

&lt;p&gt;&lt;strong&gt;Which patterns were more difficulty to learn and why?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We defined three estimates of pattern difficulty, which turned out to correlate strongly with each other (rho from .85 to .93):&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;how many people learned the pattern&lt;/li&gt;
&lt;li&gt;the number of correct predictions averaged across people&lt;/li&gt;
&lt;li&gt;the point of insight (i.e., how early people identified the pattern)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;They are shown below as &amp;ldquo;B: Learned&amp;rdquo;, &amp;ldquo;C: Correct predictions&amp;rdquo;, and &amp;ldquo;D: Point of insight.&amp;rdquo;&lt;/p&gt;

&lt;p&gt;Figure 3: (&lt;a href=&#34;https://anselmrothe.github.io/mole_cogsci2020/img/fig3.pdf&#34;&gt;PDF version&lt;/a&gt;)
&lt;img src=&#34;https://anselmrothe.github.io/mole_cogsci2020/img/fig3.png&#34; alt=&#34;Results&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Finally, we aimed to predict pattern difficulty from the features of their specific graphical programs (e.g., how often &lt;em&gt;turn&lt;/em&gt; occurred).&lt;/p&gt;

&lt;p&gt;We regressed these features onto whether a participant had learned a pattern or not in a mixed-effects logistic regression, with random intercepts for participants. &amp;ldquo;E: Regression&amp;rdquo; in the figure above shows the pattern difficulty estimates generated from the regression model.&lt;/p&gt;

&lt;p&gt;Predictive of more difficult patterns was the number of explicitly encoded terms in the program code (e.g., &lt;em&gt;turn&lt;/em&gt;, &lt;em&gt;repeat&lt;/em&gt;, &lt;em&gt;i&lt;/em&gt;, &lt;em&gt;jump&lt;/em&gt;). The repeat command allows for compressed program representations and the loop counter variable &lt;em&gt;i&lt;/em&gt; enables the iterative growing of pattern parts such as in spirals.&lt;/p&gt;

&lt;h3 id=&#34;discussion&#34;&gt;Discussion&lt;/h3&gt;

&lt;p&gt;Some patterns were generally easier to learn than others, a difficulty grade that we were able to trace back to features of the programs expressing the patterns. Pattern difficulty was in part predicted by features reflecting the program complexity, such as the number of explicitly encoded movements, and the program’s compressibility, exploited by repetitions. These are typical features of programming code, but could not be identified with traditional rule- or function-learning approaches (e.g., Schulz et al., 2017).&lt;/p&gt;

&lt;p&gt;In ongoing work, we are developing a model of the pattern learning process, based on a search through the space of possible graphical programs.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;References&lt;/strong&gt;
&lt;div style=&#34;font-size: small;&#34;&gt;&lt;/p&gt;

&lt;p&gt;
Amalric, M., Wang, L., Pica, P., Figueira, S., Sigman, M., &amp; Dehaene, S. (2017). The language of geometry: Fast comprehension of geometrical primitives and rules in human adults and preschoolers. PLoS computational biology, 13(1), e1005273.
&lt;/p&gt;

&lt;p&gt;
Fodor, J. A. (1975). The language of thought (Vol. 5). Harvard University Press.
&lt;/p&gt;

&lt;p&gt;&lt;p&gt;
Schulz, E., Tenenbaum, J. B., Duvenaud, D., Speekenbrink, M., &amp;amp; Gershman, S. J. (2017). Compositional inductive biases in function learning. Cognitive Psychology, 99, 44–79.
&lt;p&gt;&lt;/p&gt;

&lt;p&gt;&lt;/div&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Asking goal-oriented questions and learning from answers</title>
      <link>https://anselmrothe.github.io/publication/2019cogsci_questions/</link>
      <pubDate>Mon, 01 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://anselmrothe.github.io/publication/2019cogsci_questions/</guid>
      <description>

&lt;!-- More detail can easily be written here using *Markdown* and $\rm \LaTeX$ math code. --&gt;

&lt;h4 id=&#34;abstract&#34;&gt;Abstract&lt;/h4&gt;

&lt;p&gt;The study of question asking in humans and machines has gained attention in recent years. A key aspect of question ask- ing is the ability to select good (informative) questions from a provided set. Machines —in particular neural networks— generally struggle with two important aspects of question asking, namely to learn from the answer to their selected question and to flexibly adjust their questioning to new goals. In the present paper, we show that people are sensitive to both of these aspects and describe a unified Bayesian account of question asking that is capable of similar ingenuity. In the first experiment, we predict people’s judgments when adjusting their question-asking towards a particular goal. In the second experiment, we predict people’s judgments when deciding what follow-up question to ask. An alternative model based on superficial features, such as the existence of certain key words in the questions, was not able to capture these judgments to a reasonable degree.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Do people ask good questions?</title>
      <link>https://anselmrothe.github.io/publication/2018questions/</link>
      <pubDate>Fri, 03 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>https://anselmrothe.github.io/publication/2018questions/</guid>
      <description>

&lt;!-- More detail can easily be written here using *Markdown* and $\rm \LaTeX$ math code. --&gt;

&lt;h4 id=&#34;abstract&#34;&gt;Abstract&lt;/h4&gt;

&lt;p&gt;People ask questions in order to efficiently learn about the world. But do people ask good questions? In this work, we designed an intuitive, game-based task that allowed people to ask natural language questions to resolve their uncertainty. Question quality was measured through Bayesian ideal-observer models that considered large spaces of possible game states. During free-form question generation, participants asked a creative variety of useful and goal-directed questions, yet they rarely asked the best questions as identified by the Bayesian ideal-observers (Experiment 1).  In subsequent experiments, participants strongly preferred the best questions when evaluating questions that they did not generate themselves (Experiments 2 &amp;amp; 3). On the one hand, our results show that people can accurately evaluate question quality, even when the set of questions is diverse and an ideal-observer analysis has large computational requirements. On the other hand, people have a limited ability to synthesize maximally-informative questions from scratch, suggesting a bottleneck in the question asking process.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Successful structure learning from observational data</title>
      <link>https://anselmrothe.github.io/publication/2018causalstructure/</link>
      <pubDate>Thu, 02 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>https://anselmrothe.github.io/publication/2018causalstructure/</guid>
      <description>

&lt;!-- More detail can easily be written here using *Markdown* and $\rm \LaTeX$ math code. --&gt;

&lt;h4 id=&#34;abstract&#34;&gt;Abstract&lt;/h4&gt;

&lt;p&gt;Previous work suggests that humans find it difficult to learn the structure of causal systems given observational data alone. We identify two conditions that enable successful structure learning from observational data: people succeed if the underlying causal system is deterministic, and if each pattern of observations has a single root cause. In four experiments, we show that either condition alone is sufficient to enable high levels of performance, but that performance is poor if neither condition applies. A fifth experiment suggests that neither determinism nor root sparsity takes priority over the other. Our data are broadly consistent with a Bayesian model that embodies a preference for structures that make the observed data not only possible but probable.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Topics and trends in cognitive science</title>
      <link>https://anselmrothe.github.io/publication/2018cogsci_dtm/</link>
      <pubDate>Mon, 02 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>https://anselmrothe.github.io/publication/2018cogsci_dtm/</guid>
      <description>

&lt;!-- More detail can easily be written here using *Markdown* and $\rm \LaTeX$ math code. --&gt;

&lt;h4 id=&#34;abstract&#34;&gt;Abstract&lt;/h4&gt;

&lt;p&gt;What are the major topics of the Cognitive Science Society conference? How have they changed over the years? To answer these questions, we applied an unsupervised learning algorithm known as dynamic topic modeling (Blei &amp;amp; Lafferty, 2006) to the 2000–2017 Proceedings of the Cognitive Science Society. Unlike traditional topic models, a dynamic topic model is sensitive to the temporal context of documents and can characterize the evolution of each topic across years. Using this model, we identify historical trends in the popularity of topics over time, and shifts in word use within topics indicative of changing focuses within the field. We also measure the correlation across topics, and use the model to highlight the topic structure of particular papers and labs. We believe dynamic topic models present an important tool towards understanding Cognitive Science as it continues to grow and evolve over time.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Grounding compositional hypothesis generation in specific instances</title>
      <link>https://anselmrothe.github.io/publication/2018cogsci_zendo/</link>
      <pubDate>Sun, 01 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>https://anselmrothe.github.io/publication/2018cogsci_zendo/</guid>
      <description>

&lt;!-- More detail can easily be written here using *Markdown* and $\rm \LaTeX$ math code. --&gt;

&lt;h4 id=&#34;abstract&#34;&gt;Abstract&lt;/h4&gt;

&lt;p&gt;A number of recent computational models treat concept learning as a form of probabilistic rule induction in a space of language-like, compositional concepts.  Inference in such models frequently requires repeatedly sampling from a (infinite) distribution over possible concept rules and comparing their relative likelihood in light of current data or evidence.  However, we argue that most existing algorithms for top-down sampling are inefficient and cognitively implausible accounts of human hypothesis generation.  As a result, we propose an alternative, Instance Driven Generator (IDG), that constructs bottom-up hypotheses directly out of encountered positive instances of a concept. Using a novel rule induction task based on the children&amp;rsquo;s game Zendo, we compare these &amp;ldquo;bottom-up&amp;rdquo; and &amp;ldquo;top-down&amp;rdquo; approaches to inference.  We find that the bottom-up IDG model accounts better for human inferences and results in a computationally more tractable inference mechanism for concept learning models based on a probabilistic language of thought.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Modeling second-language learning from a psychological perspective</title>
      <link>https://anselmrothe.github.io/publication/2018duolingo/</link>
      <pubDate>Sun, 01 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>https://anselmrothe.github.io/publication/2018duolingo/</guid>
      <description>

&lt;!-- More detail can easily be written here using *Markdown* and $\rm \LaTeX$ math code. --&gt;

&lt;h4 id=&#34;abstract&#34;&gt;Abstract&lt;/h4&gt;

&lt;p&gt;Psychological research on learning and memory has tended to emphasize small-scale laboratory studies. However, large datasets of people using educational software provide opportunities to explore these issues from anew perspective. In this paper we describe our approach to the Duolingo Second Language Acquisition Modeling (SLAM) competition which was run in early 2018. We used a well known class of algorithms (gradient boosted decision trees), with features partially informed by theories from the psychological literature. After detailing our modeling approach and a number of supplementary simulations, we reflect on the degree to which psychological theory aided the model, and the potential for cognitive science and predictive modeling competitions to gain from each other.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>https://anselmrothe.github.io/post/dtm/</link>
      <pubDate>Sun, 01 Apr 2018 00:00:00 +0200</pubDate>
      
      <guid>https://anselmrothe.github.io/post/dtm/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Question asking as program generation</title>
      <link>https://anselmrothe.github.io/publication/2017nips/</link>
      <pubDate>Sun, 03 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>https://anselmrothe.github.io/publication/2017nips/</guid>
      <description>

&lt;!-- More detail can easily be written here using *Markdown* and $\rm \LaTeX$ math code. --&gt;

&lt;h4 id=&#34;abstract&#34;&gt;Abstract&lt;/h4&gt;

&lt;p&gt;A hallmark of human intelligence is the ability to ask rich, creative, and revealing questions. Here we introduce a cognitive model capable of constructing human-like questions. Our approach treats questions as formal programs that, when executed on the state of the world, output an answer. The model specifies a probability distribution over a complex, compositional space of programs, favoring concise programs that help the agent learn in the current context. We evaluate our approach by modeling the types of open-ended questions generated by humans who were attempting to learn about an ambiguous situation in a game. We find that our model predicts what questions people will ask, and can creatively produce novel questions that were not present in the training set. In addition, we compare a number of model variants, finding that both question informativeness and complexity are important for producing human-like questions.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Asking and evaluating natural language questions</title>
      <link>https://anselmrothe.github.io/publication/2016cogsci/</link>
      <pubDate>Fri, 01 Jul 2016 00:00:00 +0000</pubDate>
      
      <guid>https://anselmrothe.github.io/publication/2016cogsci/</guid>
      <description>&lt;!-- More detail can easily be written here using *Markdown* and $\rm \LaTeX$ math code. --&gt;

&lt;p&gt;&lt;em&gt;See the extended version of this project &lt;a href=&#34;https://anselmrothe.github.io/publication/2018questions/&#34;&gt;&amp;rarr; here&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Causal status meets coherence: The explanatory role of causal models in categorization</title>
      <link>https://anselmrothe.github.io/publication/2012cogsci/</link>
      <pubDate>Wed, 01 Aug 2012 00:00:00 +0000</pubDate>
      
      <guid>https://anselmrothe.github.io/publication/2012cogsci/</guid>
      <description>

&lt;!-- More detail can easily be written here using *Markdown* and $\rm \LaTeX$ math code. --&gt;

&lt;h4 id=&#34;abstract&#34;&gt;Abstract&lt;/h4&gt;

&lt;p&gt;Research on causal-based categorization has found two competing effects: According to the causal-status hypothesis, people consider causally central features more than less central ones. In contrast, people often focus upon feature patterns that are coherent with the category’s causal model (coherence hypothesis). Following up on the proposal that categorization can be seen as inference to the best explanation (e.g., Murphy &amp;amp; Medin, 1985), we propose that causal models might serve different explanatory roles. First, a causal model can serve as an explanation why the prototype of a category is as it is. Second, a causal model can also serve as an explanation why an exemplar might deviate from the prototype. In an experiment, we manipulated whether typical or atypical features were linked by causal mechanism. We found a causal-status effect in the first case and a coherence effect in the latter one, suggesting both are faces of the same coin.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
